---
title: "Random Forest Project"
author: "Dinh Do"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

This paper explores the machine learning method Random Forest. Random Forest is a collection (ensemble of decision trees) and average the outcome for decision. Random Forest is used in various fields such as finance, healthcare such as Alzheimer's research, and this paper helps walk through what the methodology is, how it works, and the application using existing data set. The methodology reduces risk of over fitting, create a collection of the decision trees, and average out the results.

Machine learning aim to predict the phenomena to classify, regression without conventional guidance like computer programming. Machine learning method like Random Forest helps solve human pose detection for Microsoft Kinect. [@gilles2015]

#### Understanding Random Forests: From Theory to Practice

This article covers the theory and explores the inner methods and limitations of random forest. The articles explained the application of the random forest and machine learning in general. In fields such as science or finance, machine learning aim to derive insights from the past to predict the future. From the theory, the goal is to interpret the model and predict the phenomenon for the future. However, there are limitations with Random Forest. [@gilles2015]

There is a gap with what machine learning methodology able to produce and what the end user was able to extract. It is as important for end user to trust in the result, understand the process, and interpret the results. [@gilles2015]. Machine learning state of the art methodology create a gap of understanding with the interpreter.

There are 3 limitations with Random Forest:

* Random forest has practical implication that does not result from empirical theory. Random forest does prove to show concrete results, but lack the backing from theoretical knowledge.
* There are lacking in application from the random forest from the theory
* There are difficulty in interpretation the results of the model


#### A random forest guided tour Gérard Biau & Erwan Scornet

This article also shows a beginner guide to the random forest as a way of classifying data by Breiman in 2001. Breinman further refine the algorithm in 2004. The article walks through the origin of Random Forest, the success as a general purpose machine learning algorithm, and the math behind the procedure. Random forest is an ambiguous term. The methodology is an ensemble of decision tree and average out the results. 

Random Forest is used in various field. 
 

#### Machine Learning: How Decision Trees Work and How They Can Be Combined Into A Random Forest

This  book describes the foundation for Random Forest and its collection of decision trees. 
[@devon2015]. Random forest is a collection of decision tree that filter out the results based off the features. The decision tree filter out the data based off the characteristic. For example you can select the fruit based off the color. If the color is orange, then it is an orange, if not then it is a banana. Decision tree helps clarify the data based off the choice chosen. The classification can generate error if there are more than two fruits.  

Machine learning helps predict the trend of the existing data based off the existing data set. Machine learning learns from the existing data with the current results. To understand Random Forest, you need to understand Decision tree. Decision tree helps classify the data based off the existing variable.

#### Random Forest for genetic association study

This article covers how Random Forest is used by ecologist to study the microorganism. The article highlighted the flexibility of Random Forest. RF can include up to 40000 categories. RF use decision trees and find the best outcome out of the branches of the decision trees. [@goldstein2011]



#### Random forest prediction of Alzheimer’s disease using pairwise selection from time series data

The paper explores random forest and prediction of Alzheimer's disease (AD). The paper used Random Forest to predict conversion from MCI (mild cognitive impairment) to AD through pairwise comparison of time series data. The author used random forest on time series with 4 data points. Number of trees was 60. Random forest is a collection of decision tree put in set of rectangles to minimize loss function. Process stop until a stopping criterion.

This paper explores the prediction of the AD. Early detection of AD is important for medicine therapy and random forest with time series help predict the AD. The paper states that random forest is easy to train, partition the tree, and easy to navigate according to the journal. This results in 90% accuracy for random forest method classification of neuroimage data of AD vs HC (healthy control).

The best performing entry to the Kaggle Neuroimaging Challenge used an ensemble of random forest models to achieve an accuracy of 61.9% in a blind external validation dataset. RF outperform SVM and linear mixed effects. A limitation of the approach is the direct comparison with just two methods both with a small number of predictors.

#### How random is the random forest? Random forest algorithm on the service of structural imaging biomarkers for Alzheimer's disease

The author highlighted the robustness to over fitting, handling outliers, handling of non-linear data to predict conversion from MCI to AD. This resulted in the first place in the international challenge for automated prediction of MCI from MRI data. RF produced the best robustness to manipulate non-linear data, easily tuned, and process in parallel. RF can reduce the variable space by ranking the value of the feature.

The author also explain how random forest training procedure:
"At the current node, randomly select p features from available features D. The number of features p is usually much smaller than the total number of features D. Compute the best split point for tree k using the specified splitting metric (Gini Impurity, Information Gain, etc.) and split the current node into daughter nodes and reduce the number of features D from this node on. Repeat steps 1 to 2 until either a maximum tree depth l has been reached or the splitting metric reaches some extrema.
Repeat steps 1 to 3 for each tree k in the forest. Vote or aggregate on the output of each tree in the forest."

#### Landslide susceptibility estimation by random forest techniques: sensitivity and scaling issue

Landslide susceptibility mapping (LSM) survey scale of landslide condition, the resolution of the mapping unit. They predict the probability of landslides occurring in an area without taking into consideration the past occurrence of landslides. The method used in this article is Random Forest. There are three reasons for this method over another. First, we do not suffer from the classification problem of decision trees. Random forest is a machine learning method.

This article examines the landslide susceptibility LVM. The process focuses not on comparing the model, but on the validity of the methodology. You first focus the machine learning method of the Random Forest. This is a Bayesian method based on the ensemble of trees and the collection of the results. Landslide cause financial damage, life at risk, and the blockage of roads and land usage. The results shows that by combining the two method instead of relying on hermologist is better than traditional method. The original method relies on the expertise of the hermologist, but lacks consistency. Three separate teams of hermologist produced three different mapping on the same area. This article sums up that combining the two resources is the best way moving forward for creating LVM.

#### Landslide susceptibility assessment in Lianhua county (China): A comparison between a random forest mining technique and bivariate and multivariate statistical methods.

This article compares the accuracy of prediction of landslide using Lvm, random forest, and linear model. Landslides are a common phenomenon in China resulting in losses in billions of yen yearly. The research involves the determination of the correct predictors.The predictors can be the slope of the hill, the soil sample, the rain density, and the proximity to the river.
The study uses a sample of 120 slides (70%) for model building and 30% for testing the accuracy of the model. The results show that random forest shows strong prediction despite the “noisy” variable. Variables that might not predict the outcome of a landslide. Results show RF accurately predicts 75%. RF provides accuracy despite high variance.

## Methods


## Analysis and Results

This example try to predict the quality of wine based on the acid level, the ph in water, and other variables. 

### Data and Vizualisation

A study was conducted to predict the quality of wine using random forest. The quality of 8 is best and 1 is worst quality. We are trying to predict quality of wine using chlorides, sugar, and alcohol.

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 

install.packages("randomForest",repos = "http://cran.us.r-project.org")
library(randomForest)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
library(readr)
data<- read_csv("winequality-red.csv")

head(data)

hist(data$quality)
# change quality data to category data
data$quality =as.factor(data$quality)

# separate to 80% data for training, and 20% for testing
data_set_size= floor(nrow(data)*0.80)
index <- sample(1:nrow(data),size=data_set_size)
training <- data[index,]
testing <- data[-index,]

```

### Statistical Modeling

### Conlusion

## References
